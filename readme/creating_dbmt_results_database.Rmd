---
title: "Creating DBMT Results Database"
author: "Ezgi Karaesmen"
date: 'Last Updated: `r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE,  message=FALSE, error=FALSE, warning=FALSE)
```


# SNP table

## SNP Table Prep

Working directory on CCR: 

```
/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC
```

`Sanger_HRC_allChr_final.txt` has null character values in `name` column, which causes errors while character wrangling. Currently (Sep 2019) dev version of `fread()` skips nul characters by default and used to import the file.

```{r}
### data.table dev version
# install.packages("data.table", repos="https://Rdatatable.gitlab.io/data.table")

library(data.table)
library(tidyverse)

file_path <- "/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/Sanger_HRC_allChr_final.txt"
snp_tbl <- fread(file_path)
```

Because splitting the first column of the SNP file is very memory intensive a SLURM job was submitted to create `DBprep/split_snp_tbl_1to22chr.tsv` and `DBprep/split_snp_tbl_Xchr.tsv` files. Code for these can be found on GitHub `code/snp_tbl_split.R` and `code/snp_tbl_split_sjob.sh`. 

R script `code/snp_tbl_split.R`     
1. Splits the first column `name` by chromosome, position, rsID, reference allele and alternative allele.    
2. Saves SNPs with missing rsIDs to `DBprep/missing_snps_1to22chr.tsv` and `DBprep/missing_snps_Xchr.tsv`.

## Retrieving missing rsIDs

Get all dbsnp 151 SNPs in vcf format from dbsnp ftp via [ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/](ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/)

```{bash}
cd /projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/all_snps_dbsnp_151_GRCh37
wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/All_20180423.vcf.gz
wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/All_20180423.vcf.gz.md5
wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/All_20180423.vcf.gz.tbi
```


These files were saved under 

```
/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/all_snps_dbsnp_151_GRCh37
```

Missing SNPs were then queried, cleaned by Kris (Wang) and saved to:

```
/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/missingsnpRsidDict_cleaned_1to22chr.txt

/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/missingsnpRsidDict_cleaned_Xchr.txt
```
**NOTE**     
About ~100 SNPs retrieved two rsIDs, of these the older rsID was kept as these were merged to older SNPs. (Yes sounds quite unusual, but what can you do...)           
     


All details of the querying steps can be found in:

```
/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/snpQuery
```

## Fix missing SNPs

Working directory

```
/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep
```

Script `snp_tbl_fix_missing_rsIDs.R` adds queried SNPs with missing rsIDs and also deals with SNPs that have no rsIDs even after querying. This was a memory intensive job and was submitted as a SLURM job with `snp_tbl_fix_missing_rsIDs_sjob.sh`.

Output of these steps are saved as `clean_snp_tbl_1to22chr.tsv` and `clean_snp_tbl_Xchr.tsv`


## Diagnostics

```{r}
snp_tbl <- vroom::vroom("/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/clean_snp_tbl_1to22chr.tsv")

chrx <-  vroom::vroom("/projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/clean_snp_tbl_Xchr.tsv")
```

Total number of snps

```{r}
nrow(snp_tbl)
```

```
39131578
```

```{r}
nrow(chrx)
```

```
1228034
```

SNPs with 0 MAF

```{r}
snp_tbl %>% 
  filter(BMT_MAF ==0 | BMT_MAF == 1) %>%
  nrow()
```

```
7247568
```

Low frequency SNPs

```{r}
snp_tbl %>% 
  filter(BMT_MAF < 0.001 | BMT_MAF > 1-0.001) %>%
  nrow()
```

```
24402430
```

Low quality SNPs

```{r}
snp_tbl %>% 
  filter(INFO < 0.3) %>%
  nrow()
```

```
6373749
```

Check indels

```{r}
ref_indel <- nchar(snp_tbl$ref) > 1
sum(ref_indel)
```

```
0
```

```{r}
alt_indel <- nchar(snp_tbl$alt) > 1
sum(alt_indel)
```

```
0
```

No indels!


### SNPs with missing rsID

Even after querying there were SNPs with no rsID. These SNPs were saved with rsID `[CHR]:[POS]_[REF]_[ALT]` to not allow the `rsid` column to have missing values in the database.

```{r}
snp_tbl %>%
  filter(!str_detect(rsid, "^rs")) %>%
  head(3)
```

```
# A tibble: 3 x 10
  snp_str                           chr    pos rsid         ref   alt   typed
  <chr>                           <dbl>  <dbl> <chr>        <chr> <chr> <lgl>
1 1_83108_1:83108_C/G_C_G_FALSE       1  83108 1:83108_C/G  C     G     FALSE
2 1_565999_1:565999_C/A_C_A_FALSE     1 565999 1:565999_C/A C     A     FALSE
3 1_701763_1:701763_C/G_C_G_FALSE     1 701763 1:701763_C/G C     G     FALSE
```

```{r}
snp_tbl %>%
  filter(!str_detect(rsid, "^rs")) %>%
  nrow()
```

```
743635
```


## Wrangling Details

All below listed wrangling steps have been completed in with the `snp_tbl_fix_missing_rsIDs.R` script as mentioned above. Some important steps are:

1. Change `typed` col from "TYPED" or "UNTYPED" to `TRUE` or `FALSE`    
This is better for the database, Boolean takes *much* less space than a 5/7 character column.   

2. Unite `name` with `typed` to create `snp_str` this column now has the format:   
    **"10_98087_rs11252127_C_T_TRUE"**        
    
3. For SNPs with missing rsID, replace `.` in snp_str with `[CHR]:[POS]_[REF]/[ALT]` (e.g. 10:118527881_C/T) format for each SNP. This is how these SNPs's rsIDs are represented in the result files. So in order to retrieve matching hashed IDs the string should be a perfect match!

NOTE: This `snp_str` column will then be hashed with MD5 algorithm creating `snp_id` for database. This column should be unique at all times and CANNOT have any duplicated values!

```{r}
anyDuplicated(snp_tbl$snp_str)
```

```
[1] 0
```

No duplicated snp_str!


**NOTE**     
Tri-allelic SNP example

```
                        snp_str chr    pos      rsid ref alt typed  publicMAF
1 10_129082_rs9419542_C_G_FALSE  10 129082 rs9419542   C   G FALSE 0.05056980
2 10_129082_rs9419542_C_T_FALSE  10 129082 rs9419542   C   T FALSE 0.00117031
     BMT_MAF     INFO
1 0.05327020 0.959468
2 0.00155371 0.805528
```

While rsIDs are the same, `snp_str` is unique for both.

## Transfer files to cloud

Send both files (e.g. `clean_snp_tbl_1to22chr.tsv` and `clean_snp_tbl_Xchr.tsv`) to cloud location `/data/DBMT_results_database/snp_tbl/`

```{bash}
scp /projects/rpci/lsuchest/lsuchest/Rserve/BMT/genetic_data/PLINK2VCF/Sanger_HRC/DBprep/clean_snp_tbl_* ezgikara@199.109.192.37:/data/DBMT_results_database/snp_tbl/
```

## Pre-database prep - Adding hashed IDs

Working directory on cloud: 

```
/projects/DBMT_results_database/snp_tbl
```

See `/data/DBMT_results_database/code/insert_snp_tbl.R` for actual code.


```{r}
snp_tbl <- vroom::vroom("good_snps.txt")
```


Following is the example of inserting CHR1 SNPs into the snp table.     
See `/projects/DBMT_results_database/code/insert_snp_tbl.R` for actual code.

```{r}
snp_tbl_chr <- snp_tbl[snp_tbl$chr == 1,]
snp_tbl_chr$snp_id <- openssl::md5(snp_tbl_chr$snp_str)
head(snp_tbl_chr)
```


```
  snp_str     chr    pos rsid  ref   alt   typed publicMAF BMT_MAF  INFO snp_id
  <chr>     <dbl>  <dbl> <chr> <chr> <chr> <lgl>     <dbl>   <dbl> <dbl> <chr>
1 1_752566…     1 752566 rs30… G     A     TRUE     0.820   0.821  1     eada7a…
2 1_752721…     1 752721 rs31… A     G     TRUE     0.814   0.813  1     3ce3ff…
3 1_753269…     1 753269 rs61… C     G     FALSE    0.997   0.997  0.849 5ab9b4…
4 1_753405…     1 753405 rs31… C     A     FALSE    0.853   0.850  0.945 88deac…
5 1_753541…     1 753541 rs20… G     A     FALSE    0.151   0.152  0.939 cddb52…
6 1_754063…     1 754063 rs12… G     T     FALSE    0.0409  0.0306 0.667 6a5b2f…
```

```{r}
class(snp_tbl_chr$snp_id)
```

```
"hash" "md5"
```

### Attention!

It is very important to keep any additional `snp_str` in `[CHR]_[POS]_[RSID]_[REF]_[ALT]_[TYPED(boolean)]` format e.g. `10_98087_rs11252127_C_T_TRUE` ! This is how the hash is created and this unique ID will be used as foreign key on the results table. 


## Create table

Table was created on postgres side with the following SQL code:

```{sql}
create table snp (
  snp_id UUID NOT NULL PRIMARY KEY,
	RSID VARCHAR(50) NOT NULL,
	CHR	SMALLINT NOT NULL,
	POS INTEGER NOT NULL,
  REF CHAR(1),
  ALT CHAR(1),
  TYPED BOOLEAN,
	INFO REAL,
	public_MAF REAL,
	DBMT_MAF REAL
);


```


## Insert SNP Table 

Connect to database

```{r}
require(odbc)
require(DBI)

con <- DBI::dbConnect(odbc::odbc(),
                      driver = "PostgreSQL Unicode",
                      database = "dbmt_results",
                      UID    = "postgres",
                      PWD    = "password",
                      host = "localhost",
                      port = 5432)
dbListTables(con)
```


Drop `snp_str` column, not needed for the database and insert to `snp` table.

```{r}
snp_tbl_chr <- dplyr::select(snp_tbl_chr,
                             snp_id,
                             rsid,
                             chr,
                             pos,
                             ref, 
                             alt, 
                             typed,
                             info=INFO,
                             public_maf=publicMAF,
                             dbmt_maf = BMT_MAF)
dbWriteTable(con, "snp", snp_tbl_chr, append=TRUE)
```

# Outcome Table

## Outcome Table Prep

### Transfer files to cloud

```{bash}
scp /projects/rpci/lsuchest/lsuchest/DBMT_PhenoData/DBMT_PhenoData_EA_long_allVar_20190223.txt ezgikara@199.109.192.6:/data/DBMT_results_database/outcome_tbl/

scp /projects/rpci/lsuchest/lsuchest/DBMT_PhenoData/mismatch_phenotype_wide_100d_20190223.txt ezgikara@199.109.192.6:/data/DBMT_results_database/outcome_tbl/
```


### Table Prep in R

See `/data/DBMT_results_database/code/insert_outcome_tbl.R`

Working directory on cloud:

```
/data/DBMT_results_database
```

All code below is to wrangle the data to get it ready for database insertion.    
It also counts the number of events and samples per cohort, disease group etc.  
Any further insertions doesn't need to follow the same code. Modify as appropriate.

```{r}
library(tidyverse)
pheno_rd <- vroom::vroom("outcome_tbl/DBMT_PhenoData_EA_long_allVar_20190223.txt")
pheno_mm <- vroom::vroom("outcome_tbl/mismatch_phenotype_wide_100d_20190223.txt") %>%
  drop_na(sample_type) %>%
  mutate(sample_type = "MM")

pheno_rd %>%
  bind_rows(pheno_mm) %>%
  select(sample_type, 
         cohort,
         disease,
         contains("_1Y"),
         contains("_100d"),
         -contains("S_1Y"),
         -intxsurv_1Y, -intxrel_1Y, -other_1Y,
         lfs_1Y) -> outcm_tbl0

outcm_tbl1 <- outcm_tbl0 %>%
  mutate(disease = "mixed")

outcm_tbl2 <- outcm_tbl0 %>%
  filter(disease %in% c("AML", "MDS")) %>%
  mutate(disease = "AMLMDS")

outcm_tbl0 %>%
  bind_rows(outcm_tbl1, 
            outcm_tbl2) -> outcm_tbl0

outcm_tbl <- 
  outcm_tbl0 %>%
  group_by(sample_type,
           cohort,
         disease) %>%
  summarise(n = n(), 
            OS_1y = sum(dead_1Y),
            TRM_1y = sum(TRM_1Y),
            DRM_1y = sum(disease_death_1Y),
            PFS_1y = sum(lfs_1Y),
            ORGF_1y = sum(OF_1Y),
            GVHD_1y = sum(GVHD_death_1Y),
            INF_1y = sum(infection_1Y),
            OS_100d = sum(dead_100d),
            DRM_100d = sum(drm_100d),
            TRM_100d = sum(trm_100d),
            PFS_100d = sum(lfs_100d),
            GVHD_100d = sum(gvhd_100d),
            INF_100d = sum(inf_100d),
            ORGF_100d = sum(of_100d)) %>%
  ungroup()  %>%
  gather(key=outcome, value=nevent, -(sample_type:n)) %>%
  gather("key", "value", n, nevent) %>%
  mutate(cohort= paste0("c", cohort)) %>%
  unite(key, key, cohort) %>%
  spread(key, value) %>%
  separate(outcome, c("outcome", "censor_time")) %>%
  mutate(genome = recode(sample_type,
                         donor="D",
                         recipient="R")) %>%
  mutate(ethnicity = "EA",
         pt_subset = as.character(NA)) %>%
  select(genome, outcome, disease_grp=disease, everything(), -sample_type) 

```


### Add hashed IDs

Again the order to generate `outcome_str` is very important as this will be used to generate the unique `outcome_id`s!

```{r}
outcm_tbl %>%
  unite(outcome_str, 
        genome, disease_grp, outcome, censor_time,ethnicity, pt_subset,
        remove = FALSE) -> outcm_tbl

write_tsv(outcm_tbl, "outcome_tbl/outcome_tbl.tsv")

head(outcm_tbl)
```

```{r, echo=FALSE, eval=TRUE}
library(tidyverse)
library(knitr)
outcm_tbl <- read_tsv("outcome_tbl/outcome_tbl.tsv")
head(outcm_tbl, 3) %>% kable 
```

Finally add the md5 generated hashed IDs.

```{r}
outcm_tbl %>%
  mutate(outcome_id = openssl::md5(outcome_str)) %>%
  select(-outcome_str) -> outcm_tbl
```


### How columns are coded (currently)

```{r, echo=FALSE, eval=TRUE}
outcm_tbl %>%
  select(-(n_c1:nevent_c2), - outcome_str) %>%
  as.list() %>%
  map(., function(x) paste(unique(x), collapse = ", ")) -> tbllst

tibble(
  Columns = names(tbllst),
  Categories = unlist(tbllst)) %>%
  kable()
```


## Create Outcome Table 

```{sql}
create table outcome (
  outcome_id UUID NOT NULL PRIMARY KEY,
  genome VARCHAR(2) NOT NULL,
  outcome	VARCHAR(10) NOT NULL,
  censor_time VARCHAR(10) NOT NULL,
  disease_grp VARCHAR(15) NOT NULL,
  pt_subset VARCHAR(50),
  ethnicity VARCHAR(5) NOT NULL,
  N_c1 SMALLINT,
  N_c2 SMALLINT,
  Nevent_c1 SMALLINT,
  Nevent_c2 SMALLINT
);
```


## Insert Outcome Table

```{r}
require(DBI)
require(odbc)

con <- DBI::dbConnect(odbc::odbc(),
                      driver = "PostgreSQL Unicode",
                      database = "dbmt_results",
                      UID    = "postgres",
                      PWD    = "password",
                      host = "localhost",
                      port = 5432)
dbListTables(con)


dbWriteTable(con, "outcome", outcm_tbl, append=TRUE)
```


# Results Table

## Data Prep

File processing and tranfer to cloud on the CCR side are done with `code/result_tbl_process_transfer.R` and were submitted for all files under each results directory with `code/result_tbl_process_transfer_sjob.sh`. Both of these scripts are available on the GitHub repository and at `/projects/rpci/lsuchest/lsuchest/DBMT_results/DBprep/code`. All intermediary files can be found under `/projects/rpci/lsuchest/lsuchest/DBMT_results/DBprep/`.   
       

Both scripts do the following:     
- For a given `out` directory (e.g. `/projects/rpci/lsuchest/lsuchest/DBMT_results/DBMT_mixed/analyses/mixed_EA_results/out`) it submits a job for each file (these should be `.res` files) under that directory.    
- During each job the file is processed to be inserted to the database and named accordingly to ease the database insertion in the cloud side.    
- Upon completing the processing it transfers the file to the cloud side `data/DBMT_results_database/results_tbl`. 

**Important notes:**

1. For file transfer to work ssh keys should be set up by the user. This is explained in detail below.    

1. R script should be modified depending on the group of the results to reflect the cencoring time, patient ethnicity and further specifics about the analysis. These are used to name the file and assigned to `save_file_as` variable in the R script.


### Transfer files to cloud

This part is executed in the R script with:

```{r, eval=FALSE}
system(glue::glue("scp {path_out}/{save_file_as} ezgikara@199.109.192.37:/data/DBMT_results_database/results_tbl/"))
```

But user needs to set up ssh keys to skip the password requirement for file transfer. To get the ssh keys:

Generate private/public ssh key using `ssh-keygen` on CCR. It is not necessary to set up a password (so just click enter twice) after invoking `ssh-keygen`. The public key will be saved in `~/.ssh/id_rsa.pub`. Copy the contents of the public ssh key and paste it in a flat file `~/.ssh/authorized_keys` on the remote server (e.g. in your `.ssh` directory on cloud). If that file does not exist, create `authorized_keys` file (it does not exist by default). 

## Create Table

```{sql}
create table results (
	PRIMARY KEY (snp_id, outcome_id),
	snp_id UUID REFERENCES snp (snp_id),
	outcome_id UUID REFERENCES outcome (outcome_id),
	alt_metal CHAR(1),
  coef_c1 REAL,
  coef_c2 REAL,
  coef_m REAL,
  se_coef_c1 REAL,
  se_coef_c2 REAL,
  se_coef_m REAL,
  pvalue_c1_nlog10 REAL,
  pvalue_c2_nlog10 REAL,
  pvalue_m_nlog10 REAL,
  samp_freq_alt_c1 REAL,
  samp_freq_alt_c2 REAL,
  hetisq REAL,
  hetpval REAL);
```

## Insert Table

On cloud via R, add UUIDs and insert results.

```{r}
library(tidyverse)
library(DBI)
library(odbc)
options(tibble.width = Inf)

con <- DBI::dbConnect(odbc::odbc(), driver = "PostgreSQL Unicode", 
    database = "dbmt_results", UID = "postgres", PWD = "password", 
    host = "localhost", port = 5432)

dbListTables(con)

res_files <- dir("results_tbl")
res_file <- res_files[1]

chr_res <- vroom::vroom(paste0("results_tbl/", res_file))

chr_res <- chr_res %>%
  mutate(outcome_str = str_remove(res_file, "_CHR[0-9]+.res"),
         snp_id = openssl::md5(snp_str),
         outcome_id = openssl::md5(outcome_str)) %>%
  select(-snp_str, -outcome_str)
  
colnames(chr_res) <- tolower(gsub("[.]", "_", colnames(chr_res)))

odbc::dbWriteTable(con, "results", chr_res[5,], append=TRUE)

chr_res[5, "snp_id"] == openssl::md5(snp_str)

snp_tbl <- tbl(con, "snp")
snp_chr10 <- snp_tbl %>% 
  filter(chr == 10) %>%
  data.frame()
  
snp_chr10 <- mutate(snp_chr10, snp_id = tolower(gsub("-", "", snp_id)))

chr_res %>%
  mutate(snp_id=as.character(snp_id)) %>% 
  anti_join(snp_chr10) -> miss_snps

miss_snps %>% head

snpids_chr10 <- snp_chr10 %>% pull(snp_id)
chr_res %>%
  filter(!as.character(snp_id) %in% snpids_chr10) -> miss_snps
setdiff(as.character(chr_res$snp_id), snpids_chr10)


head(snp_chr10)
head(chr_res)



```




# eQTL Table




